package operator.dwd;import bean.TableConfig;import com.alibaba.fastjson.JSON;import com.alibaba.fastjson.JSONObject;import com.alibaba.ververica.cdc.connectors.mysql.MySQLSource;import com.alibaba.ververica.cdc.connectors.mysql.table.StartupOptions;import com.alibaba.ververica.cdc.debezium.DebeziumSourceFunction;import common.Constants;import common.process.TableProcessFunction;import common.serialization.UDFSerialization;import common.sink.hbaseSinkFunction;import org.apache.flink.api.common.state.MapStateDescriptor;import org.apache.flink.streaming.api.datastream.*;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer;import org.apache.flink.streaming.connectors.kafka.KafkaSerializationSchema;import org.apache.flink.util.OutputTag;import org.apache.kafka.clients.producer.ProducerRecord;import utils.CKUtils;import utils.KafkaUtils;import javax.annotation.Nullable;public class BaseDBOperator {    public static void main(String[] args) throws Exception {        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();        env.setParallelism(1);//        CKUtils.setCk(env);        // 读取kafka ods_base_db数据        FlinkKafkaConsumer<String> kafkaConsumer = KafkaUtils.makeKafkaConsumer(Constants.KAFKA_ODS_DB_TOPIC, Constants.KAFKA_DB_GROUP_ID);        DataStreamSource<String> dbStream = env.addSource(kafkaConsumer);        // 将每行数据转换为JSON对象 有@FunctionalInterface函数式接口注解才能写lambda表达式        SingleOutputStreamOperator<JSONObject> jsonDBStream = dbStream.map(JSON::parseObject);        // 过滤空值数据(删除数据)        SingleOutputStreamOperator<JSONObject> filterStream = jsonDBStream.filter(jsonData -> {            String data = jsonData.getString("data");            return data != null && data.length() > 0;        });        // FlinkCdc读取配置表，创建广播流，默认配置表不修改，不删除，只会新增        DebeziumSourceFunction<String> configSource = MySQLSource.<String>builder()                .hostname(Constants.HOSTNAME)                .port(Constants.PORT)                .username(Constants.USERNAME)                .password(Constants.PASSWORD)                .databaseList("config_db")                .startupOptions(StartupOptions.initial())                .deserializer(new UDFSerialization())                .build();        DataStreamSource<String> configStream = env.addSource(configSource);        // 创建描述器        MapStateDescriptor<String, TableConfig> broadcastMapStateDescriptor = new MapStateDescriptor<>("broadcastMapStateDescriptor", String.class, TableConfig.class);        // 创建广播流        BroadcastStream<String> broadcastStream = configStream.broadcast(broadcastMapStateDescriptor);        // 连接广播流和主流        BroadcastConnectedStream<JSONObject, String> connectedStream = filterStream.connect(broadcastStream);        // 创建侧输出流标签        OutputTag<JSONObject> hbaseOutputTag = new OutputTag<JSONObject>("hbase") {        };        // 先处理广播流数据，发送至主流，主流根据广播流数据处理自身数据  map一定要返回一个值，process不需要        SingleOutputStreamOperator<JSONObject> kafkaDataStream = connectedStream.process(new TableProcessFunction(hbaseOutputTag, broadcastMapStateDescriptor));        // 打印数据        kafkaDataStream.print("kafka>>>>>>");        // 从主流中获取写入到Hbase的侧输出流        DataStream<JSONObject> hbaseDataStream = kafkaDataStream.getSideOutput(hbaseOutputTag);        hbaseDataStream.print("hbase>>>>>>");        // 流里面有来自各个表的数据，必须自定义sink将HBase流写入Hbase        hbaseDataStream.addSink(new hbaseSinkFunction());        // 将Kafka流写入Kafka        kafkaDataStream.addSink(KafkaUtils.makeKafkaProducerBySchema(new KafkaSerializationSchema<JSONObject>() {            @Override            public ProducerRecord<byte[], byte[]> serialize(JSONObject element, @Nullable Long timestamp) {                return new ProducerRecord<byte[], byte[]>(element.getString("sinkTable"), element.getString("data").getBytes());            }        }));        // 启动        env.execute();    }}